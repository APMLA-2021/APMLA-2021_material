{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Common imports \n",
    "from ast import literal_eval\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from categorical_em import CategoricalEM\n",
    "import sys\n",
    "# print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 1. Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "K = 5 # Number of mixture components\n",
    "I = 120 # Number of words in the dictionary\n",
    "N = None # Number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the data\n",
    "\n",
    "First, we need to load the data from the csv. This file contains the documents already processed and cleaned after applying the following steps:\n",
    "\n",
    "1. Tokenization\n",
    "2. Homogeneization, which includes:\n",
    "    1. Removing capitalization.\n",
    "    2. Removing non alphanumeric tokens (e.g. punktuation signs)\n",
    "    3. Stemming/Lemmatization.\n",
    "3. Cleaning\n",
    "4. Vectorization\n",
    "\n",
    "\n",
    "We load it as a `pandas` dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_cleaned.csv')\n",
    "df.drop_duplicates(subset=\"tweet\", inplace=True)\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(literal_eval) #Transform the string into a list of tokens\n",
    "X_tokens = list(df['tokens'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns: {}\\n'.format(' | '.join(df.columns.values)))\n",
    "\n",
    "print('Tweet:\\n{}'.format(df.loc[1, 'tweet']))\n",
    "print('Tweet cleaned:\\n{}'.format(df.loc[1, 'tweets_clean']))\n",
    "print('Tweet tokens:\\n{}'.format(X_tokens[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dictionary\n",
    "\n",
    "Up to this point, we have transformed the raw text collection in a list of documents stored in `X_tokens`, where each document is a collection \n",
    "of the words that are most relevant for semantic analysis. Now, we need to convert these data (a list of token lists) into \n",
    "a numerical representation (a list of vectors, or a matrix). To do so, we will start using the tools provided by the `gensim` library. \n",
    "\n",
    "As a first step, we create a dictionary containing all tokens in our text corpus, and assigning an integer identifier to each one of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(X_tokens)\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words (BoW): Numerical version of documents\n",
    "In the second step, let us create a numerical version of our corpus using the `doc2bow` method. In general, \n",
    "`D.doc2bow(token_list)` transforms any list of tokens into a list of tuples `(token_id, n)`, one per each token in \n",
    "`token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences \n",
    "of such token in `token_list`. \n",
    "\n",
    "*Exercise:* Apply the `doc2bow` method from gensim dictionary `D`, to all tokens in every document in `X_tokens`. \n",
    "The result must be a new list named `X_bow` where each element is a list of tuples `(token_id, number_of_occurrences)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_bow = list()\n",
    "keep_tweet = list()\n",
    "for tweet in X_tokens:\n",
    "    tweet_bow = dictionary.doc2bow(tweet)\n",
    "    if len(tweet_bow) > 1:\n",
    "        X_bow.append(tweet_bow)\n",
    "        keep_tweet.append(True)\n",
    "    else:\n",
    "        keep_tweet.append(False)\n",
    "\n",
    "df_data = df[keep_tweet]\n",
    "\n",
    "N = len(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the BoW representation `X_bow` into a matrix, namely `X_matrix`, in which the i-th row and j-th column represents the \n",
    "number of occurrences of the j-th word of the dictionary in the i-th document. This will be the matrix used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_matrix = np.zeros([N, I])\n",
    "for i, doc_bow in enumerate(X_bow):\n",
    "    word_list = list()\n",
    "    for word in doc_bow:\n",
    "        X_matrix[i, word[0]] = word[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Categorical Mixture Model with Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Analytical forms of the E and M steps for the EM-Algorithm\n",
    "1. Write the log joint distribution: $\\log p(\\{\\mathbf{x}_n, z_n\\}| \\Theta) = ?$\n",
    "2. Write the analytical expression for $Q(\\Theta, \\Theta^{\\text{old}}) = ?$\n",
    "3. Write the MLE for $\\Theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\{\\mathbf{x}_n, z_n\\}| \\Theta) =  \\cdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_p():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2\n",
    "\n",
    "\\begin{align}\n",
    "Q(\\Theta, \\Theta^{\\text{old}}) = \\cdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3\n",
    "\\begin{align}\n",
    "\\hat{\\pi}_k = \\cdots\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\theta}_{km} = \\cdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Data anlysis task\n",
    "#### Exercise 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "K = 5 # Number of mixture components\n",
    "i_theta = 5 # Dirichlet parameter from which the parameter is sampled for initialization\n",
    "i_pi = 0 # Dirichlet parameter from which the parameter is sampled for initialization\n",
    "\n",
    "model = CategoricalEM(K, I, N, delta=0.01, epochs=200, init_params={'theta': i_theta, 'pi': i_pi})\n",
    "model.fit(X_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def AIC():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization imports \n",
    "%matplotlib inline\n",
    "import probvis.aux as pva\n",
    "import matplotlib.pyplot as plt\n",
    "pva.activate_latex_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.3\n",
    "\n",
    "Some useful packages:\n",
    "- matplotlib https://matplotlib.org/\n",
    "- seaborn https://github.com/mwaskom/seaborn\n",
    "- wordcloud https://github.com/amueller/word_cloud\n",
    "- probvis https://github.com/psanch21/prob-visualize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gmm_em_complete.categorical_em import CategoricalEM\n",
    "K = 5 # for example\n",
    "i_theta = 5\n",
    "i_pi = 5\n",
    "model = CategoricalEM(K, I, N, delta=0.01, epochs=200, init_params={'theta': i_theta, 'pi': i_pi})\n",
    "model.fit(X_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_array = np.array(df_data['tweet'].values)\n",
    "\n",
    "# Show the 10 most representative words for each topic using a cloud of words\n",
    "model.show_title_by_topic(tweet_array, model.theta_matrix, model.r_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.theta_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.r_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 10 most relevant documents for each topic.\n",
    "model.show_words_by_topic(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of Q over the epochs\n",
    "model.plot_result(dictionary, close=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
