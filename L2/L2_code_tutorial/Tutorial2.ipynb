{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Common imports \n",
    "from ast import literal_eval\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from categorical_em import CategoricalEM\n",
    "#from gmm_em.categorical_em import CategoricalEM\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gmm_em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 1. Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "K = 5 # Number of mixture components\n",
    "I = 120 # Number of words in the dictionary\n",
    "N = None # Number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the data\n",
    "\n",
    "First, we need to load the data from the csv. This file contains the documents already processed and cleaned after applying the following steps:\n",
    "\n",
    "1. Tokenization\n",
    "2. Homogeneization, which includes:\n",
    "    1. Removing capitalization.\n",
    "    2. Removing non alphanumeric tokens (e.g. punktuation signs)\n",
    "    3. Stemming/Lemmatization.\n",
    "3. Cleaning\n",
    "4. Vectorization\n",
    "\n",
    "\n",
    "We load it as a `pandas` dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../L2_code/tweets_cleaned.csv')\n",
    "df.drop_duplicates(subset=\"tweet\", inplace=True)\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(literal_eval) #Transform the string into a list of tokens\n",
    "X_tokens = list(df['tokens'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: tweet_id | timestamp | user_id | tweet | tweets_clean | tokens\n",
      "\n",
      "Tweet:\n",
      "OSINT people - please retweet, if possible. My friend is looking for women involved in OSINT. https://twitter.com/manisha_bot/status/1181594280336531457 …\n",
      "Tweet cleaned:\n",
      "osint people   please retweet  if possible  my friend is looking for women involved in osint\n",
      "Tweet tokens:\n",
      "['osint', 'peopl', 'retweet', 'possibl', 'friend', 'look', 'woman', 'involv', 'osint']\n"
     ]
    }
   ],
   "source": [
    "print('Columns: {}\\n'.format(' | '.join(df.columns.values)))\n",
    "\n",
    "print('Tweet:\\n{}'.format(df.loc[1, 'tweet']))\n",
    "print('Tweet cleaned:\\n{}'.format(df.loc[1, 'tweets_clean']))\n",
    "print('Tweet tokens:\\n{}'.format(X_tokens[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dictionary\n",
    "\n",
    "Up to this point, we have transformed the raw text collection in a list of documents stored in `X_tokens`, where each document is a collection \n",
    "of the words that are most relevant for semantic analysis. Now, we need to convert these data (a list of token lists) into \n",
    "a numerical representation (a list of vectors, or a matrix). To do so, we will start using the tools provided by the `gensim` library. \n",
    "\n",
    "As a first step, we create a dictionary containing all tokens in our text corpus, and assigning an integer identifier to each one of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12243 unique tokens: ['collin', 'cum', 'domin', 'phil', 'room']...)\n",
      "Dictionary(120 unique tokens: ['look', 'peopl', 'woman', 'love', 'work']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(X_tokens)\n",
    "print(dictionary)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=I)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words (BoW): Numerical version of documents\n",
    "In the second step, let us create a numerical version of our corpus using the `doc2bow` method. In general, \n",
    "`D.doc2bow(token_list)` transforms any list of tokens into a list of tuples `(token_id, n)`, one per each token in \n",
    "`token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences \n",
    "of such token in `token_list`. \n",
    "\n",
    "*Exercise:* Apply the `doc2bow` method from gensim dictionary `D`, to all tokens in every document in `X_tokens`. \n",
    "The result must be a new list named `X_bow` where each element is a list of tuples `(token_id, number_of_occurrences)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_bow = list()\n",
    "keep_tweet = list()\n",
    "for tweet in X_tokens:\n",
    "    tweet_bow = dictionary.doc2bow(tweet)\n",
    "    if len(tweet_bow) > 1:\n",
    "        X_bow.append(tweet_bow)\n",
    "        keep_tweet.append(True)\n",
    "    else:\n",
    "        keep_tweet.append(False)\n",
    "\n",
    "df_data = df[keep_tweet]\n",
    "\n",
    "N = len(df_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the BoW representation `X_bow` into a matrix, namely `X_matrix`, in which the i-th row and j-th column represents the \n",
    "number of occurrences of the j-th word of the dictionary in the i-th document. This will be the matrix used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_matrix = np.zeros([N, I])\n",
    "for i, doc_bow in enumerate(X_bow):\n",
    "    word_list = list()\n",
    "    for word in doc_bow:\n",
    "        X_matrix[i, word[0]] = word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 2., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Categorical Mixture Model with Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi$$\n",
    "$$\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1\n",
    "\n",
    "\\begin{align}\n",
    "p(\\{\\mathbf{x}_n\\}| \\Theta) =  \\cdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_p(X_matrix, pi_vector, theta_matrix):\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2\n",
    "\n",
    "\\begin{align}\n",
    "Q(\\Theta, \\Theta^{\\text{old}}) = \\cdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3\n",
    "\\begin{align}\n",
    "\\hat{\\pi}_k = \\cdots\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\theta}_{km} = \\cdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
